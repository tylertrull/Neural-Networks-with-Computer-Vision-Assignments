# -*- coding: utf-8 -*-
"""TRULL_Assignment4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RIMHBgSNAepjsnXxB8-OomGwqdsiLabD
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib as plt

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()


#Took the first 6000 images instead of the whole set so the program doesn't take an eternity to get through one epoch
x_train = x_train[:6000]
y_train = y_train[:6000]

#Normalizes our pixel values by dividing them all by 255 and then converting them into floats for our convolutional layer
x_train_norm = (x_train/255.0).astype('float32')
x_test_norm = (x_test/255.0).astype('float32')

#Function to create our model
def create_model(kernel, activation_function, learningrate, kernel_size):
  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.Conv2D(32,kernel_size, activation=activation_function, kernel_initializer=kernel, padding = 'same',input_shape=[32,32,3]))
  model.add(tf.keras.layers.Conv2D(32,kernel_size, activation=activation_function, kernel_initializer=kernel, padding = 'same'))
  model.add(tf.keras.layers.MaxPooling2D(2,2))
  model.add(tf.keras.layers.Dropout(0.1)) #Using Dropout for regularization
  model.add(tf.keras.layers.Flatten())
  model.add(tf.keras.layers.Dense(16, activation=activation_function, kernel_initializer=kernel))
  model.add(tf.keras.layers.Dense(10, activation='softmax'))
  opt = tf.keras.optimizers.SGD(learning_rate=learningrate, momentum=0.9)
  model.compile(optimizer=opt , loss='sparse_categorical_crossentropy', metrics=['accuracy'])
  return model


#Visualize the first 9 images of our train data
fig = plt.pyplot.figure()

for i in range(9):
  # define subplot
  ax = fig.add_subplot(330 + 1 + i)
  ax.set_title(y_train[i])
  plt.pyplot.subplots_adjust(hspace =0.6)
  # plot raw pixel data
  plt.pyplot.imshow(x_train[i])

plt.pyplot.show()

LowLearningRate = create_model('he_normal', 'relu', 0.01, (3,3))
HighLearningRate = create_model('he_normal', 'relu', 0.9, (3,3))
SigmoidActivation = create_model('he_normal', 'sigmoid', 0.001, (3,3))
HeUniformInitalizer = create_model('he_uniform', 'relu', 0.01, (3,3))
NotNormalized = create_model('he_normal', 'relu', 0.01, (3,3))
Highepoch = create_model('he_normal', 'relu', 0.01, (3,3))
HigherKernelSize = create_model('he_normal', 'relu', 0.01, (5,5))


#Creates a model using a low learning rate
print("Training with a low learning rate")
LowLearningRate.fit(x_train_norm, y_train, epochs=3, batch_size = 32, validation_data=(x_test, y_test))
#Creates a model using a high learning rate
print("Training with a high learning rate")
HighLearningRate.fit(x_train_norm, y_train, epochs=3, batch_size = 32, validation_data=(x_test, y_test))
#Creates a model using the sigmoid function as our activation and softmax for classification
print("Training using sigmoid instead of relu as activation")
SigmoidActivation.fit(x_train_norm, y_train , epochs=3, batch_size = 32, validation_data=(x_test, y_test))
#Creates a model using unnormalized values
print("Training using non normalized values")
NotNormalized.fit(x_train, y_train, epochs=3, batch_size = 32, validation_data=(x_test, y_test))
#Creates a model using a different initiazlier
print("Training using 'he_uniform' initializer instead of 'he_normal'")
HeUniformInitalizer.fit(x_train_norm, y_train, epochs=3, batch_size = 32, validation_data=(x_test, y_test))
#After observing the creation of our models, the lowlearning rate model seemed to end with the best accuracy so I used a low learning rate with higher epochs
#Higher epochs test Overfitting seemed to occur around ~5-7 epochs so I decided to go with 6 over 3
#there seems to be some overfitting still around the 6th epoch mark but accuracy and val_accuracy is still on the rise while loss decreases BUT val_loss increases a lot which are signs of overfitting
print("Training using 6 epochs over 3 ")
Highepoch.fit(x_train_norm, y_train, epochs=6, batch_size=32, validation_data=(x_test, y_test))
print("Training using a 5x5 kernel size instead of a 3x3 kernel size")
#Creates a model using a higher kernel size and the same parameters as our Low Learning Rate test
HigherKernelSize.fit(x_train_norm, y_train, epochs=6, batch_size=32, validation_data=(x_test, y_test))

#Seems our best Test was a higher epoch with a low learning rate using 'he_normal' as an initializer, relu as our activation, softmax as our classification, and a kernel size of 3x3.