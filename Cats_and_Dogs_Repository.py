# -*- coding: utf-8 -*-
"""TRULL_Assignment5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wzx0cQ_f3uDg3_AE0SJjx9ffj3S76qLW
"""

import tensorflow as tf
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin =_URL, extract=True)

PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

#Sets train directory from path
train_dir = os.path.join(PATH, 'train')
#Sets validation directory from validation
validation_dir = os.path.join(PATH, 'validation')

#Create cats
train_cats_dir = os.path.join(train_dir, 'cats')

#Create dogs
train_dogs_dir = os.path.join(train_dir, 'dogs')

#Creates our convolutional neural network with 2 conv layers, a dropout layer for regularization, a flatten layer, and two dense layers for classification
my_model = tf.keras.models.Sequential()
my_model.add(tf.keras.layers.Conv2D(16, 3, activation='relu',input_shape=[150,150,3]))
my_model.add(tf.keras.layers.Conv2D(32, 3, activation='relu'))
my_model.add(tf.keras.layers.MaxPooling2D(2,2))
my_model.add(tf.keras.layers.Dropout(0.1))
my_model.add(tf.keras.layers.Flatten())
my_model.add(tf.keras.layers.Dense(16, activation='relu'))
my_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
my_model.summary()
my_model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])

#Scales our Train data and validation data for training and validation
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale =1./255)
val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale =1./255)

#Generates the images from our directories, sets their sizes to (150,150), and inputs 20 images at a time for a batch_size 
generate_train_imgs = train_datagen.flow_from_directory(train_dir, target_size=(150, 150), batch_size = 20, class_mode='binary')
generate_valid_imgs = val_datagen.flow_from_directory(validation_dir, target_size=(150, 150), batch_size = 20, class_mode='binary')

#Fits our model with our generated image set
#Since there are 2000 images there will be 100 steps_per_epoch (100 * 20(batch_size))
my_model_history = model.fit(generate_train_imgs, steps_per_epoch=100, epochs=10, validation_data=generate_valid_imgs, validation_steps=50, verbose=2)

#Gets the mobilenet trained model and weights, sets input_shape to 224,224,3, and removes top of model
net_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=[224,224,3])

#Gets the model
x = net_model.output
#Pooling on top of the model
x = tf.keras.layers.GlobalAveragePooling2D()(x)
#Adds our own 3 dense layers on top of the model and a final dense layer for classification
x = tf.keras.layers.Dense(64, activation = 'relu')(x)
x = tf.keras.layers.Dense(128, activation = 'relu')(x)
x = tf.keras.layers.Dense(32, activation = 'relu')(x)
preds = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)
#Creates the model
model = tf.keras.Model(inputs=net_model.input,outputs = preds)


model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])

#Sets majority of the layers to nontrainable as to train only the few dense layers we added
#I removed the check for layers before running and can not be bothered to have this run for another 30 minutes
#Here is the code I used to check the amount of layers
#for i in range(len(model.layers)):
# print(model.layers[i])
#This code printed out 150 layers + the final 4 that I added so I froze those 150 layers as to not remove their predesignated weights
for layer in model.layers[:150]:
  layer.trainable=False

#Fits and trains the model
history = model.fit(generate_train_imgs, steps_per_epoch=100, epochs=10, validation_data=generate_valid_imgs, validation_steps=50, verbose=2)

#Here is where I compare our values on one chart
#These two accuracies are validation and train accuracy for the mobilenet modle
trainaccuracy = history.history['accuracy']
valaccuracy = history.history['val_accuracy']
#These two accuracies are my model's accuracies
my_trainaccuracy = my_model_history.history['accuracy']
my_valaccuracy = my_model_history.history['val_accuracy']

#For plot purposes create the amount of epochs
epochs = range(1,11)
#Make the plot bigger
plt.figure(figsize=(10,5))

#Plots our accuracy values and gives them a color and label
plt.plot(epochs, trainaccuracy, 'g', label='Mobile Net Training accuracy')
plt.plot(epochs, valaccuracy, 'b', label='Mobile Net Validation accuracy')
plt.plot(epochs, my_trainaccuracy, 'r', label='My Model Training accuracy')
plt.plot(epochs,my_valaccuracy, 'y', label='My Model Validation accuracy')

#Creates a title, sets the xlabel, sets the ylabel, and shows the plot
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()